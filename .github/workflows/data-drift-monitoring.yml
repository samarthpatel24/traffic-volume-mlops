name: Data Drift Monitoring

on:
  schedule:
    # Run every day at 3 AM UTC
    - cron: '0 3 * * *'
  
  workflow_dispatch:
    inputs:
      reference_data_path:
        description: 'Path to reference dataset'
        required: false
        default: 'data/raw/Metro_Interstate_Traffic_Volume.csv'
      drift_threshold:
        description: 'Drift detection threshold (0.0-1.0)'
        required: false
        default: '0.1'

jobs:
  drift-detection:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install evidently  # For drift detection
    
    - name: Create drift detection script
      run: |
        cat > drift_monitor.py << 'EOF'
        import pandas as pd
        import numpy as np
        from datetime import datetime, timedelta
        import json
        import sys
        from scipy import stats
        
        def load_reference_data(file_path):
            """Load reference dataset"""
            try:
                df = pd.read_csv(file_path)
                print(f"Reference data loaded: {df.shape}")
                return df
            except Exception as e:
                print(f"Error loading reference data: {e}")
                return None
        
        def simulate_current_data(reference_df, days_back=7):
            """Simulate current data (in real scenario, this would be your production data)"""
            # For demo, we'll simulate by sampling from the reference data
            # with some modifications to demonstrate drift detection
            
            n_samples = min(1000, len(reference_df))
            current_data = reference_df.sample(n=n_samples, random_state=42).copy()
            
            # Simulate some drift in temperature (slightly higher values)
            current_data['temp'] = current_data['temp'] + np.random.normal(2, 1, len(current_data))
            
            # Simulate drift in traffic volume (slightly lower values)
            current_data['traffic_volume'] = current_data['traffic_volume'] * 0.9
            
            print(f"Current data simulated: {current_data.shape}")
            return current_data
        
        def detect_numerical_drift(reference_data, current_data, column, threshold=0.05):
            """Detect drift in numerical columns using KS test"""
            ref_values = reference_data[column].dropna()
            curr_values = current_data[column].dropna()
            
            # Kolmogorov-Smirnov test
            ks_stat, p_value = stats.ks_2samp(ref_values, curr_values)
            
            drift_detected = p_value < threshold
            
            return {
                'column': column,
                'ks_statistic': ks_stat,
                'p_value': p_value,
                'drift_detected': drift_detected,
                'reference_mean': float(ref_values.mean()),
                'current_mean': float(curr_values.mean()),
                'mean_difference': float(curr_values.mean() - ref_values.mean()),
                'reference_std': float(ref_values.std()),
                'current_std': float(curr_values.std())
            }
        
        def detect_categorical_drift(reference_data, current_data, column, threshold=0.05):
            """Detect drift in categorical columns using chi-square test"""
            ref_counts = reference_data[column].value_counts()
            curr_counts = current_data[column].value_counts()
            
            # Align categories
            all_categories = list(set(ref_counts.index) | set(curr_counts.index))
            ref_aligned = [ref_counts.get(cat, 0) for cat in all_categories]
            curr_aligned = [curr_counts.get(cat, 0) for cat in all_categories]
            
            # Chi-square test
            chi2_stat, p_value = stats.chisquare(curr_aligned, ref_aligned)
            
            drift_detected = p_value < threshold
            
            return {
                'column': column,
                'chi2_statistic': chi2_stat,
                'p_value': p_value,
                'drift_detected': drift_detected,
                'reference_categories': len(ref_counts),
                'current_categories': len(curr_counts),
                'new_categories': list(set(curr_counts.index) - set(ref_counts.index)),
                'missing_categories': list(set(ref_counts.index) - set(curr_counts.index))
            }
        
        def generate_drift_report(drift_results):
            """Generate drift detection report"""
            report = {
                'timestamp': datetime.now().isoformat(),
                'drift_detected': any(result['drift_detected'] for result in drift_results),
                'total_features_checked': len(drift_results),
                'features_with_drift': sum(1 for result in drift_results if result['drift_detected']),
                'results': drift_results
            }
            
            return report
        
        def main():
            reference_file = sys.argv[1] if len(sys.argv) > 1 else 'data/raw/Metro_Interstate_Traffic_Volume.csv'
            threshold = float(sys.argv[2]) if len(sys.argv) > 2 else 0.05
            
            print(f"Starting drift detection with threshold: {threshold}")
            
            # Load reference data
            reference_data = load_reference_data(reference_file)
            if reference_data is None:
                sys.exit(1)
            
            # Get current data (simulated for demo)
            current_data = simulate_current_data(reference_data)
            
            # Define columns to check
            numerical_columns = ['traffic_volume', 'temp', 'rain_1h', 'snow_1h', 'clouds_all']
            categorical_columns = ['weather_main', 'weather_description']
            
            drift_results = []
            
            # Check numerical columns
            for col in numerical_columns:
                if col in reference_data.columns and col in current_data.columns:
                    result = detect_numerical_drift(reference_data, current_data, col, threshold)
                    drift_results.append(result)
                    
                    if result['drift_detected']:
                        print(f"⚠️  Drift detected in {col}: p-value = {result['p_value']:.6f}")
                    else:
                        print(f"✅ No drift in {col}: p-value = {result['p_value']:.6f}")
            
            # Check categorical columns
            for col in categorical_columns:
                if col in reference_data.columns and col in current_data.columns:
                    result = detect_categorical_drift(reference_data, current_data, col, threshold)
                    drift_results.append(result)
                    
                    if result['drift_detected']:
                        print(f"⚠️  Drift detected in {col}: p-value = {result['p_value']:.6f}")
                    else:
                        print(f"✅ No drift in {col}: p-value = {result['p_value']:.6f}")
            
            # Generate and save report
            report = generate_drift_report(drift_results)
            
            with open('drift_report.json', 'w') as f:
                json.dump(report, f, indent=2)
            
            print(f"\n📊 Drift Detection Summary:")
            print(f"Features checked: {report['total_features_checked']}")
            print(f"Features with drift: {report['features_with_drift']}")
            print(f"Overall drift detected: {report['drift_detected']}")
            
            if report['drift_detected']:
                print("\n🚨 DRIFT ALERT: Data drift detected! Consider retraining the model.")
                
                # Set GitHub Actions output
                import os
                if 'GITHUB_OUTPUT' in os.environ:
                    with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                        f.write(f"drift_detected=true\n")
                        f.write(f"features_with_drift={report['features_with_drift']}\n")
                
                return 1
            else:
                print("\n✅ No significant drift detected.")
                
                import os
                if 'GITHUB_OUTPUT' in os.environ:
                    with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                        f.write(f"drift_detected=false\n")
                        f.write(f"features_with_drift=0\n")
                
                return 0
        
        if __name__ == "__main__":
            import os
            exit_code = main()
            sys.exit(exit_code)
        EOF
    
    - name: Run drift detection
      id: drift_check
      run: |
        python drift_monitor.py "${{ github.event.inputs.reference_data_path || 'data/raw/Metro_Interstate_Traffic_Volume.csv' }}" "${{ github.event.inputs.drift_threshold || '0.05' }}"
    
    - name: Create drift visualization
      if: always()
      run: |
        cat > create_drift_plots.py << 'EOF'
        import pandas as pd
        import matplotlib.pyplot as plt
        import seaborn as sns
        import json
        import numpy as np
        from datetime import datetime
        
        # Load drift report
        try:
            with open('drift_report.json', 'r') as f:
                report = json.load(f)
        except:
            print("No drift report found")
            exit(0)
        
        # Create visualizations
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Data Drift Detection Report', fontsize=16)
        
        # Plot 1: Drift detection summary
        drift_counts = {
            'No Drift': sum(1 for r in report['results'] if not r['drift_detected']),
            'Drift Detected': sum(1 for r in report['results'] if r['drift_detected'])
        }
        
        axes[0, 0].pie(drift_counts.values(), labels=drift_counts.keys(), autopct='%1.1f%%',
                      colors=['green', 'red'])
        axes[0, 0].set_title('Drift Detection Summary')
        
        # Plot 2: P-values for each feature
        features = [r['column'] for r in report['results']]
        p_values = [r.get('p_value', r.get('p_value', 0)) for r in report['results']]
        
        colors = ['red' if p < 0.05 else 'green' for p in p_values]
        axes[0, 1].bar(range(len(features)), p_values, color=colors)
        axes[0, 1].axhline(y=0.05, color='black', linestyle='--', alpha=0.7, label='Threshold')
        axes[0, 1].set_xticks(range(len(features)))
        axes[0, 1].set_xticklabels(features, rotation=45)
        axes[0, 1].set_ylabel('P-value')
        axes[0, 1].set_title('Feature Drift P-values')
        axes[0, 1].legend()
        
        # Plot 3: Mean differences for numerical features
        numerical_results = [r for r in report['results'] if 'mean_difference' in r]
        if numerical_results:
            num_features = [r['column'] for r in numerical_results]
            mean_diffs = [r['mean_difference'] for r in numerical_results]
            
            colors = ['red' if r['drift_detected'] else 'green' for r in numerical_results]
            axes[1, 0].bar(range(len(num_features)), mean_diffs, color=colors)
            axes[1, 0].set_xticks(range(len(num_features)))
            axes[1, 0].set_xticklabels(num_features, rotation=45)
            axes[1, 0].set_ylabel('Mean Difference')
            axes[1, 0].set_title('Mean Differences (Current - Reference)')
            axes[1, 0].axhline(y=0, color='black', linestyle='-', alpha=0.3)
        
        # Plot 4: Summary statistics
        axes[1, 1].text(0.1, 0.9, f"Timestamp: {report['timestamp']}", transform=axes[1, 1].transAxes)
        axes[1, 1].text(0.1, 0.8, f"Features Checked: {report['total_features_checked']}", transform=axes[1, 1].transAxes)
        axes[1, 1].text(0.1, 0.7, f"Features with Drift: {report['features_with_drift']}", transform=axes[1, 1].transAxes)
        axes[1, 1].text(0.1, 0.6, f"Overall Drift: {'Yes' if report['drift_detected'] else 'No'}", 
                        transform=axes[1, 1].transAxes, 
                        color='red' if report['drift_detected'] else 'green',
                        fontweight='bold')
        axes[1, 1].set_xlim(0, 1)
        axes[1, 1].set_ylim(0, 1)
        axes[1, 1].set_title('Detection Summary')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.savefig('drift_detection_report.png', dpi=300, bbox_inches='tight')
        print("Drift visualization saved as drift_detection_report.png")
        EOF
        
        python create_drift_plots.py
    
    - name: Upload drift report
      uses: actions/upload-artifact@v4
      with:
        name: drift-report-${{ github.run_number }}
        path: |
          drift_report.json
          drift_detection_report.png
        retention-days: 30
    
    - name: Create issue on drift detection
      if: steps.drift_check.outputs.drift_detected == 'true'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const report = JSON.parse(fs.readFileSync('drift_report.json', 'utf8'));
          
          const issueBody = `
          # 🚨 Data Drift Detected
          
          **Alert:** Data drift has been detected in the traffic volume prediction dataset.
          
          ## Summary
          - **Detection Time:** ${report.timestamp}
          - **Features Checked:** ${report.total_features_checked}
          - **Features with Drift:** ${report.features_with_drift}
          
          ## Affected Features
          ${report.results.filter(r => r.drift_detected).map(r => 
            `- **${r.column}:** p-value = ${r.p_value.toFixed(6)}`
          ).join('\n')}
          
          ## Recommended Actions
          1. 🔍 **Investigate** the cause of data drift
          2. 📊 **Analyze** the affected features in detail
          3. 🤖 **Consider retraining** the model with recent data
          4. 📈 **Monitor** model performance metrics
          5. 🔧 **Update** data preprocessing if needed
          
          ## Next Steps
          - [ ] Review drift detection report
          - [ ] Analyze root cause of drift
          - [ ] Retrain model if necessary
          - [ ] Update monitoring thresholds if appropriate
          
          This issue was automatically created by the Data Drift Monitoring workflow.
          `;
          
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `🚨 Data Drift Alert - ${new Date().toISOString().split('T')[0]}`,
            body: issueBody,
            labels: ['data-drift', 'alert', 'mlops']
          });
    
    - name: Trigger model retraining on significant drift
      if: steps.drift_check.outputs.drift_detected == 'true' && steps.drift_check.outputs.features_with_drift > 2
      uses: actions/github-script@v7
      with:
        script: |
          github.rest.actions.createWorkflowDispatch({
            owner: context.repo.owner,
            repo: context.repo.repo,
            workflow_id: 'model-training.yml',
            ref: 'main',
            inputs: {
              retrain_reason: 'Automatic retrain triggered by data drift detection',
              model_type: 'ensemble'
            }
          });